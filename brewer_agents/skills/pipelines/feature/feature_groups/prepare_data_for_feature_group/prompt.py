# from hopsworks_brewer.agents.base_prompt import SingularPrompt


# class DataPreparationPrompt(SingularPrompt):
#     system_message = (
#         "You are an expert data scientist specializing in feature engineering\n"
#         "and feature store operations. Generate your response following the exact format specified,\n"
#         "with one requirements block for installations and one Python block for all code. Ensure your code:\n"
#         "1. Uses the specified function structure with proper context and error handling\n"
#         "2. Returns string context and error in a consistent format\n"
#         "3. Properly documents all processing decisions\n"
#         "4. Validates critical columns throughout the pipeline\n"
#         "5. Generates complete feature descriptions\n"
#         "6. Follows the exact output format for JSON results"
#     )

#     human_message = (
#         "\n"
#         "Task: Create a comprehensive data preparation pipeline while preserving critical columns\n"
#         "required for feature group creation.\n"
#         "\n"
#         # region Parameters
#         "STAGE 1 CODE (Analysis; Strictly follow the data retrieval code with the correct filtering on event time (if present), don't change it):\n"
#         "{data_analysis_code}\n"
#         "\n"
#         "ANALYSIS CONTEXT:\n"
#         "{data_analysis_context}\n"
#         "\n"
#         "CURRENT CODE:\n"
#         "{stage_code}\n"
#         "\n"
#         "ERRORS IN THE CURRENT CODE (if any, focus on the last error):\n"
#         "{error_logs}\n"
#         "\n"
#         "PLAN YOU NEED TO FOLLOW:\n"
#         "{workflow_plan}\n"
#         "\n"
#         # region Requirements
#         "REQUIREMENTS:\n"
#         "1. Critical Columns Preservation:\n"
#         "- IMPORTANT: All column names should be in the lower case\n"
#         "- NEVER remove primary key or event time columns\n"
#         "- Maintain feature group essential columns:\n"
#         "    * Primary keys\n"
#         "    * Partition key (if specified)\n"
#         "    * Event time column (if present)\n"
#         "\n"
#         "2. Code Integration:\n"
#         "- Review and reuse appropriate parts of the analysis code\n"
#         "- Keep necessary import statements and data loading logic\n"
#         "- Remove analysis-specific code such as logs, etc\n"
#         "- Maintain error handling from the first stage\n"
#         "- Analyze the provided ANALYSIS CONTEXT to understand the data we are working with\n"
#         "\n"
#         "3. Package Installation:\n"
#         "- First, provide pip install requirements.txt in this format exactly:\n"
#         "```requirements\n"
#         "package1\n"
#         "package2\n"
#         "```\n"
#         "- Only include actually needed packages\n"
#         "\n"
#         "4. Data Preparation Pipeline:\n"
#         "a) Data Loading:\n"
#         "   - Reuse the robust loading logic from stage 1\n"
#         "   - Maintain existing error handling\n"
#         "   - Set pandas display options for full visibility\n"
#         "\n"
#         "b) Data Preprocessing:\n"
#         "    - Analyze the provided ANALYSIS CONTEXT. Then identify what preprocessing steps should be performed\n"
#         "    - IMPORTANT: All feature names should be in the lower case using .lower()"
#         "    Steps to consider:\n"
#         "        - Handle missing values with appropriate strategies\n"
#         "        - Implement necessary data type conversions\n"
#         "        - Create new features based on analysis insights\n"
#         "        - Address outliers and inconsistencies\n"
#         "        - Standardize formats\n"
#         "\n"
#         "c) Feature Group Preparation:\n"
#         "   - Identify and preserve primary key columns\n"
#         "   - Handle any feature-specific requirements\n"
#         "   - Validate presence of all required columns\n"
#         "\n"
#         "d) Quality Validation:\n"
#         "   - Verify transformations\n"
#         "   - Ensure all columns are visible in output display\n"
#         "\n"
#         "5. Feature Group Analysis:\n"
#         "Analyze dataset characteristics relevant for feature group creation:\n"
#         "   - Primary key candidates\n"
#         "   - Is the feature group online or historical?\n"
#         "   - Feature importance estimates\n"
#         "   - Event time column identification\n"
#         "\n"
#         "The code must follow this exact structure:\n"
#         "```python\n"
#         "import json\n"
#         "from typing import Tuple, Optional\n"
#         "import pandas as pd\n"
#         "<other required imports>\n"
#         "\n"
#         "def prepare_dataset() -> Tuple[Optional[str], Optional[str]]:\n"
#         "    '''\n"
#         "    Main preparation function that processes the dataset and returns results\n"
#         "\n"
#         "    Returns:\n"
#         "        Tuple containing:\n"
#         "        - str | None: Preparation output if successful, None if failed\n"
#         "        - str | None: Error message if failed, None if successful\n"
#         "    '''\n"
#         "    context_lines = []\n"
#         "\n"
#         "    try:\n"
#         "        # Your data loading and preprocessing code here\n"
#         "        # Use context_lines.append() for collecting output\n"
#         "\n"
#         "        # Process the data and collect results\n"
#         "        processed_df = ...  # Your data processing logic\n"
#         "\n"
#         "        # Create the feature group context\n"
#         "        feature_group_context = {{\n"
#         "            'preprocessing_summary': {{\n"
#         "                'rows_before': len(original_df),\n"
#         "                'rows_after': len(processed_df),\n"
#         "                'columns_before': len(original_df.columns),\n"
#         "                'columns_after': len(processed_df.columns),\n"
#         "                'new_features': [...],\n"
#         "                'removed_features': [...],\n"
#         "                'transformations_applied': [...]\n"
#         "            }},\n"
#         "            'feature_group_metadata': {{\n"
#         "                'primary_key_columns': [...],\n"
#         "                'event_time': '...',\n"
#         "                'feature_descriptions': {{\n"
#         "                    # Complete descriptions for ALL features\n"
#         "                }},\n"
#         "                'online_enabled': True/False,\n"
#         "                'partition_key': '...',\n"
#         "            }}\n"
#         "        }}\n"
#         "\n"
#         "        # Add processing summary to context\n"
#         '        context_lines.append("=== Data Processing Summary ===")\n'
#         "        context_lines.append(f\"Original rows: {{feature_group_context['preprocessing_summary']['rows_before']}}\")\n"
#         "        context_lines.append(f\"Processed rows: {{feature_group_context['preprocessing_summary']['rows_after']}}\")\n"
#         '        context_lines.append("\\n=== Processed Data Preview ===")\n'
#         "        context_lines.append(processed_df.head().to_string(index=False))\n"
#         '        context_lines.append("\\n=== Feature Group Context ===")\n'
#         "        context_lines.append(json.dumps(feature_group_context, indent=2))\n"
#         "\n"
#         "        return '\\\\n'.join(context_lines), None  # Success case\n"
#         "\n"
#         "    except Exception as e:\n"
#         "        return None, str(e)  # Error case\n"
#         "\n"
#         'if __name__ == "__main__":\n'
#         "    # Prepare the dataset\n"
#         "    context, error = prepare_dataset()\n"
#         "    # Print results in a format that can be parsed by the calling script\n"
#         "    result = {{\n"
#         '        "success": error is None,\n'
#         '        "context": context,\n'
#         '        "error": error\n'
#         "    }}\n"
#         "    print(json.dumps(result))\n"
#         "```\n"
#         "\n"
#         # region FORMAT REQUIREMENTS
#         "IMPORTANT FORMAT REQUIREMENTS:\n"
#         "1. Place ALL required pip install packages within a SINGLE requirements code block at the start\n"
#         "2. Use the exact format shown above for Python requirements\n"
#         "3. Place the rest of the Python code in a separate python code block\n"
#         "4. The code MUST follow the structure above with:\n"
#         "   - A main prepare_dataset function that returns (context, error)\n"
#         '   - An if __name__ == "__main__" block that outputs JSON-formatted results\n'
#         "   - Proper handling of all critical columns and validations\n"
#         "5. Make sure to preserve important columns during aggregation and data preprocessing steps.\n"
#         "\n"
#         "Generate code that follows these requirements and specifically addresses any previous errors: {error_logs}\n"
#         "The code should be production-ready, well-documented, and handle all potential edge cases."
#     )
